---
title: "Logical Open Sorts (LOS)"
author:
- "Maximilian Held"
- "Verena Kasztantowicz"
date: "`r Sys.Date()`"
bibliography: held_library.bib
abstract: >
  In Q methodology, subjectivity is measured in *closed sorts* along one or several *given* dimensions, such as a condition of instruction shared with all participants.
  Other methods, such as [Repertory Grid Technique](https://en.wikipedia.org/wiki/Repertory_grid) (RGT) and several *open sorting* techniques allow each participant to define her own dimensions, though these approaches do not share the tenets of operant subjectivity, such as an ipsative measurement and Q-mode extraction.
  We here describe a data gathering procedure and experimental analysis to unite these two features, to measure a operant subjectivity with *open sorts*, starting with logical assignments.
  
  We invite participants to come up with their own, dichotomous categories and ask them to classify the full Q-Set accordingly.
  The resultant logical table is summarized into a per-person measure of categorical item similarity, in turn transformed into a person-by-person correlation matrix.
  We then extract higher-level principal components (PCA) in a stepwise residual process, and calculate the respective (rotated) component scores, yielding ideal-typical, shared ways of categorizing the Q-Set.
  These categorical viewpoints are then subjected to a nested, low-level PCA to allow a substantive abduction of just *what* those categories might be.
  
  Using real-world data gathered for this purpose, we show that results from the suggested procedure can be meaningfully interpreted as operant subjectivity with open dimensionality.
  We also present software for the R statistics framework to run the analysis.
  
  To spontaneously and ideosyncratically categorise things and ideas encountered in the world, is perhaps a definiting operation of the human mind.
  Here too, using the right approach, we can distill shared, categorical viewpoints on these items, extending the scientific study of human subjectivity to new formats and realms.
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Logical Open Sorts}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Introduction

> "The factors are likewise natural, representing actual *categories of thinking* that are operant with respect to the issues under consideration." 
> 
> -- Steven Brown [-@Brown1980: 70, emphasis added]

<!-- TODO must add more background, literature and motivation here -->

In this procedure, participants sort items according to their own, "first-person", arbitrary categories.
<!-- TODO add citation for first-person -->
These categories are:

- **operant**, because they arise *spontaneously* as participants engage the items,
- **inductive**, because participants describe their own, *open-ended* categories, which can be neither correct, nor incorrect,
- **ipsatively** assigned, because categories are originally defined as *similarity* between categories.

In contrast to other sorting techniques, which require a *single* set of mutually exclusive, and comprehensively exhaustive ("MECE") categories as a disjoint set vis-a-vis the items [@coxon-1999, p. 3], `Q-Cat` *encourages* overlapping *multiple* categories, each of which is merely *logically* (`TRUE`, `FALSE`) assigned to each item.
This complicates the analysis, but avoids imposing a pre-supposed structure of categorical subjectivity on participants.
^[In a future iteration, participants will ipsatively *rank* items vis-a-vis a categorical *prototype* (chosen from the items, compare @rosch-1978), yielding *ordinal* information, requiring a separate, non-parametric analytical procedure.]

<!-- - not clear what this is: Multiple Sorting Procedure [@barnett-2004] -->


# Sorting Procedure

<!--TODO VK: do these files this still exist, and are they ehlpful to explain the procedure? 
[](Verena/Documents/GoogleDrive/komki/pics/qpic13.png)
[](Verena/Documents/GoogleDrive/komki/pics/qpic9.png)
[](Verena/Documents/GoogleDrive/komki/pics/qpic10.png)
[](Verena/Documents/GoogleDrive/komki/pics/qpic11.png)
[](Verena/Documents/GoogleDrive/komki/pics/qpic12.png)
[](Verena/Documents/GoogleDrive/komki/pics/qpic14.png)
-->

As in a traditional Q-sort, the LOS procedure requires items printed on paper cards.
The same criteria for good items hold. 
<!-- TODO cite section on good items; mention differences -->
For this study, the 35 items covered language games, such as the following two examples:

> **Language of Bees**  
> *Bee-german: 'Summ, summ, summ.'  
> Bee-english: 'Samm, Samm, Samm.'  
> Bee-french: 'Summe, summe, summe.'   
> Bee-finnish: 'Suomi, suomi, suomi'*  

> **Let's eat Grandpa**  
> *Let's eat grandpa.  
> Let's eat, grandpa.  
> Commas - They save lives!*

As in Q, personal administration of the sorts with one-on-one interviews are recommended to get a deeper sense about what the participants are thinking and to avoid influences between them, but online administration is also conceivable.

If the participants are not familiar with the items, you should plan enough time to let them read all the items carefully or to listen to an audio version of, as we have done with the children in this study to make sure they have experienced the whole text.
Sometimes you can gather some interesting spontaneous reactions here (laughter, incomprehension or refusal), to whom you could come back later in the interview.

As a starting question, we asked the participants to take two items, which seems similar to them (in any aspect) and put them to the side.
We asked them to describe in what way the texts are similar and what feature they share.
We then noted this category description in a paper table, sorted by an category index (say, from `A` to `Z`).
Starting out with pairs of similar items, naturally, increases the minimum number of *shared* categories: this procedure will never yield a category which applies only to a *single* item.
This "bias" is defensible, because it flows from what we want to measure: categorical *similarity* between the items.
<!-- must explain more clearly, that this degree of minimal similary is, in fact, by design -->
A single-item category produces no additional information in this regard, and it is unclear how a category with a membership of one would be meaningful.

After each paired comparison, we invited the participating children to find *more* items, for which the category would apply, putting them to one side of the table.
Once all items have been considered for the category in question, the index of that category (say, `C`) is noted on all cards, for which the category applied.
All cards are then mixed again, placed in the center of the table, and the process begins again, with a new pair of similar items.

```{r process-pics, echo=FALSE, out.width="100%", fig.cap=c("Participant considers all items.", "Participant chooses two similar items, names category.", "Participant applies categories to all items.", "All items are categorized as TRUE or FALSE."), dpi = 72, fig.show = "asis"}
knitr::include_graphics(path = c("q-cat-stage-0.jpg", "q-cat-stage-1.jpg", "q-cat-stage-2.jpg", "q-cat-stage-3.jpg"))
```

Ensuring that any one category is assessed for *all* items can be mentally taxing for the participants, but is essential for the downstream analysis.
We also tried an alternative procedure, where participants first define all categories, and then check all items for each of the categories.
This can sometimes be a little faster, but the categories cannot be as easily changed, and the ipsative nature of the assessment may be lost.
Whatever the order, the tedium of assessing *all* categories on *all* items remains.
This tiring step in LOS is an unavoidable disadvantage of the approach, and compares unfavorably to Q-sorts, which participants often enjoy completing.
^[The comparison is a bit unfair, because LOS produces a lot more information then a Q-sort.]

The participating children sometimes had a hard time explaining a category, mixing several characteristics or simply describing items as "strange".
This is to be expected, perhaps in any age group, because spontaneous categorisations are rarely well-defined.
If participants so choose, they can revise their category descriptions at any point, but these can also remain imperfect.
LOS is an attempt to measure categorical similarity between the items, and as such, the categories may well *be* vague and ill-defined -- that may be just the operant subjectivity.
Accordingly, the category descriptions do not actually figure in the downstream analysis, they merely serve in the final interpretation to make sense of the extracted factors of categorical similarity.


# Data Storage

<!-- TODO this would appear to be wrong; ass in canonical form is a LIST of matrices, not an array -->
For our example study on categorizations of language games with children and grown-ups, this yields a *list* of description matrices (one element for each participant, as in table \@ref(tab:desc-example)) and a three-dimensional *array* of assignment matrices (one slice for each participant, as in table \@ref(tab:ass-example)).
^[This canonical data representation can be easily produced from conveniently entered raw data with [`pensieve::import_qcat()`](http://pensieve.maxheld.de).]

```{r knitr-setup}
knitr::opts_chunk$set(eval = FALSE)
```

```{r julius-desc, echo = FALSE}
# prepare some example subset
chosen_cats <- c(A = "Tiere",
                 L = "Aussprache",
                 H = "Satzzeichen",
                 G = "Reime")
chosen_cat_indices <- which(x = LETTERS %in% names(chosen_cats))
komki$qcat$desc[names(chosen_cats), "Julius"] <- c("animals", 
                                                   "pronounciation", 
                                                   "punctuation", 
                                                   "rhymes")

chosen_items <- c("language-of-bees", "eating-grandpa")

desc_example <- data.frame(Index = chosen_cat_indices,
                           Description = komki$qcat$desc[names(chosen_cats), "Julius"],
                           row.names = NULL)

ass_example <- komki$qcat$ass$Julius[chosen_items, chosen_cat_indices]
```

```{r desc-example, echo=FALSE}
kable(x = desc_example,
      row.names = FALSE,
      caption = "Description Matrix (Subset)")
```

```{r ass-example, echo=FALSE}
kable(x = ass_example,
      row.names = TRUE, 
      col.names = as.character(c(1:ncol(ass_example))),
      caption = "Assignment Matrix (Subset)")
```


```{r setup-q-cat, echo = FALSE, message=FALSE}
library(knitr)
opts_knit$set(echo = FALSE,
              cache = TRUE,
              fig.retina = 2,
              dpi = 72) 

library(devtools)
library(rmarkdown)
library(ggplot2)
library(reshape2)
library(plyr)
library(grid)
library(gridExtra)
library(tufte)
library(xtable)
options(xtable.comment = FALSE)
options(xtable.booktabs = TRUE)
options(scipen=999)

install_github(repo = "maxheld83/qmethod")
library(qmethod)  # from github
```


```{r kill-nas, echo=FALSE}
data(komki)

# kill nas
komki$qsorts <- na.omit(komki$qsorts)  # because some items were never shown to some people
#TODO this deletion may no longer be necessary as per newest job procedures

komki$items <- komki$items[komki$items$Handle.english %in% rownames(komki$qsorts), ]
komki$qcat$ass <- sapply(X = komki$qcat$ass, simplify = FALSE, FUN = function(x) {
  x[rownames(x) %in% rownames(komki$qsorts), , drop = FALSE]
})

# killing jason, because a) he just didn't REALLY do the thing and b) he dominates the qcat model, which we don't like.
komki$qsorts <- komki$qsorts[, colnames(komki$qsorts) != "Jason"]
komki$qcat$desc <- komki$qcat$desc[, colnames(komki$qcat$desc) != "Jason"]
komki$qcat$ass$Jason <- NULL
```





## Analysis 1: Shared Categories as Surprising Similarity

To analyze `Q-Cat` data, we must first render the individual categorisations comparable.
 ^[Note that the data in canonical form *cannot* be compared between individuals. For example, `Nhome`'s first category is independently defined (by her) from the above `Julius`'s first category, and so on.]
To do that, we first transform the binary assignments into continuous deviations from *probable* assignments.
The probable assignment is the *expected value* $\mathbb{E}$ for some item draw, which is, intuitively, the probability-weighted ($p$) arithmetic average of outcomes $x_1$ (`TRUE`)  and $x_2$ (`FALSE`),

<!-- TODO use pretty math (doesn't work with with tufte-html for now) -->
<!-- <script type="text/x-mathjax-config"> -->
<!-- MathJax.Hub.Config({ -->
<!--   TeX: { equationNumbers: { autoNumber: "AMS" } } -->
<!-- }); -->
<!-- </script> -->
<!-- \begin{equation} -->
<!--   \mathbb{E}(X) = x_1 p_1 + x_2 p_2 \label{eq:ev} -->
<!-- \end{equation} -->

$$\mathbb{E}(X) = x_1 p_1 + x_2 p_2$$

where $p_1$ probability of `TRUE` is simply the count of `TRUE`s $z$ divided by the number of items $y$, $p_1 = z / y$, and $p_2 = 1 - p_1$.
We then subtract this *expected value* from the *observed* realization for some $x$, yielding

$$x{'} = x - \mathbb{E}(X).$$



```{r make-surprise, echo=FALSE}
make_surprise <- function(ass) {
  surprise <- ass  # TODO better make this object empty in future!
  for (person in names(ass)) {
    ev <- colSums(ass[[person]]) / nrow(ass[[person]])
    surprise[[person]] <- t(apply(X = ass[[person]], MARGIN = 1, FUN = function(x) {x - ev}))
  }
  return(surprise)
}
surprise <- make_surprise(ass = komki$qcat$ass)
kable(x = surprise$Julius[chosen_items, chosen_cat_indices],
      col.names = as.character(c(1:4)),
      caption = "Assignments as Surprisal Values (Subset)")
```

We can now express `Julius`' above assignments from table \@ref(tab:ass-example) as $x{'}$, an information-theoretical *surprisal value* [@attneave-1959].
^[Our measure is a greatly simplified version of *Burton's $Z$*, which required *conditional* probabilities for item-pair co-occurences, because items are drawn into MECE categories *without* replacement [compare @burton-1972].]
A high positive value, such `Julius`' value for category `1` on `language-of-bees` indicates that this assignment is *positively* surprising, given the probable assignment; it's `TRUE`"ishness" is *higher* than would be expected on average.
The inverse holds for `Julius`' value for category `2` on `eating-grandpa`; it is *less* `TRUE`ish than would be expected, even though only slightly so.

Summary statistics about the surprisal value matrices are also readily interpretable.
For example, `Julius` has a mean surprisal value of `mean(surprise$Julius["language-of-bees", ])` for `language-of-bees`, implying that the item attracted many *more* category assignments than expected.
Conversely, a high standard deviation, such as for `Julius`' `the-same` (`sd(surprise$Julius["the-same", ])`) suggests that the item was assigned much *more* than expected to some categories, but not to others.
Both characteristics of category assignments are appropriately standardized away by the correlation coefficient, because a high center of, or high spread of assignments should not give extra weight to some item.
<!-- TODO this is still a little thin, but also maybe just a footnote -->

Thus standardized for the category *width*, *spread* and *center* we can now easily *correlate* the surprisal value of all item pairs, yielding a three dimensional array of items x items x people.
^[A simpler approach, tried out earlier, would simply *count* the co-occurences of item-pairs in any set of categories, but such a procedure does not standardize for category width, and has the disadvantage of only producing a *co-occurence* matrix.]

This correlation of the *surprisal values* of item pairs, observed over a (varying) number of (open-ended) categories is, oddly, neither an `R`, nor a `Q`-type analysis.
The correlated variables are items, but the observations are *also* "variables" of sorts, namely the inductive categories described by participants.
As will be obvious in the next step, this preliminary summary is necessary to enable a "Q-way" analysis of the categorical data available here: categorisations must *first* be made comparable accross participants, which is what the surprisal value correlation matrices as a rough indication of *categorically* assigned similarity accomplish.

`Julius` slice is display in figure \@ref(fig:make-cora).
The correlation coefficients encompass a surprising range, all the way from `-1` to `1` - even on the off-diagonal.
Strictly speaking, the values *can* be interpreted as categorically assigned, *surprising* similarity.
*Measured by the granularity of the present study* (i.e. the number of observed categories for some participant), an off-diagonal `1` can be taken to indicate *total* similarity.
As with other samples, this measure entails an element of chance: `Julius`' *perfect* correlation between items `resistance` and `comma` likely does *not* indicate that `Julius` thought the two were truly *identical*.
They just *appear* to be identical on the (limited) number categories observed, and would probably be differentiated, had they been observed on more, or different categories.
We can, consequently, have more confidence in a surprisal correlation matrix that is based on a greater number of observation (= categories), because chance "identities" are less likely to arise, though given the intensive nature of the method, the number of observations is likely to always remain quite limited.
When extracting the *shared* patterns of categorical similarity, it will be important to deflate resulting models by the probability of such random, likely false-positive identities through means of a custom parallel analysis or related methods [@Glorfeld-1995, @Horn-1965].

This operation appears, at first glance, similar to Repertory Grid Technique [RGT, e.g. @fransella-2004], where participants also evaluate a *given* set of items (called "elements" in RGT) on some inductive, participant-defined categories (called "constructs" in RGT), though RGT employs *interval* measurements (not categorical) and cannot reveal inter-individual *differences*, because the analysis procceeds R-ways.
The analysis suggested here, works quite differently - observations and variables are, in classic Q fashion, transposed.
Whereas in RGT, open-ended *categories* are correlated over *items* as observations to reveal similarity categories, we - initially - suggest to correelate *items* over categories as observations to reveal similar items, which are, at a later stage, referred back to initially entered categories.

```{r make-cora, echo=FALSE, fig.cap="Julius' Item x Item Correlation Matrix as a Heatmap", fig.height=10, fig.width=10}
make_cora <- function(surprise) {
  cora <- sapply(X = surprise, USE.NAMES = TRUE, simplify = "array", FUN = function(x) {
    m <- cor(t(x))
    return(m)
  })
  names(dimnames(cora)) <- c("item", "item", "people")
  return(cora)
}
cora <- make_cora(surprise = surprise)

GGally::ggcorr(data = cora[,,"Julius"], label = TRUE)
```

The correlation heatmap in \@ref(fig:make-cora) is broadly informative, but too big for researchers to make sense of, simply because the item combinations are many - as they should be, for a productive analysis.
Because item surprisal similarity is here expressed as a simple correlation matrix, we can employ a Principal Components Analysis (PCA) to reduce its dimensionality.

```{r julius-pca, echo=FALSE, fig.cap="Item Loadings on Julius' First Two Quartimax-Rotated Components"}
pca_julius <- prcomp(x = t(surprise$Julius), retx = TRUE, center = TRUE, scale. = TRUE)
# todo add a proper biplot here, does that even make sense?
library(GPArotation)
rotated_julius <- quartimax(L = pca_julius$rotation[,1:7])$loadings

# now let's find the scores, so we know which were the original categories assigned here
scores_julius <- matrix(data = NA,
                        nrow = ncol(rotated_julius),
                        ncol = ncol(surprise$Julius))
for(pc in 1:ncol(rotated_julius)) {
  scored_surprise <- surprise$Julius
  for(item in rownames(rotated_julius)) {
    scored_surprise[item, ] <- surprise$Julius[item, ] * rotated_julius[item, pc]
  }
  scores_julius[pc,] <- colSums(scored_surprise)
}

scores_n_desc <- data.frame(desc = komki$qcat$desc[1:17,"Julius"],
                           scores = t(scores_julius))
# View(scores_n_desc)

#rownames(rotated_julius) <- komki$items$Handle.deutsch  # comment me out unless you want german items
ggplot(data = as.data.frame(rotated_julius[,1:2]), mapping = aes(x = PC1, y = PC2, label = rownames(rotated_julius))) + geom_text(position = position_jitter(width = 0.05))
```

Figure \@ref(fig:julius-pca) displays the item loadings in the first two rotated principal components (out of seven with an Eigenvalue greater than one).
These loadings can be interpreted as similarity of items in terms of their surprising category assignment; `i-we` and `but-how` *both* are surprisingly *present* on the first dimension of such similarity, while `riddle` and `idiom` are both surprisingly *absent* on the same dimension.
Using factor *scores*, which are here ideal-typical category *assignments*, we can also relate this summary back to the original descriptions.
A cursory inspection of the item pattern and the underlying descriptions suggests that `Julius` first rotated reflects his formal categorisations (such as punctuation), as opposed to his more substantive judgments (such as whether an item was a joke, or played with the meaning of words).
Such *individual* level summary illustrates the logic and *should* be meaningful, in principle, though it is likely to be of limited use in real research because the underlying observations are so sparse, and uncorrected surprisal values accordingly unreliable for an individual.
^[A proper analysis of individual level categorisations will also benefit from more specialized visualizations and may require custom rotation methods.]


## Analysis 2: Ideal Types of Ideal Types

We now have an array $\underline{X}$ of order

$$J \times J \times K$$

or, in this context,

$$Items \times Items \times People$$,

where cells cells are Pearson's correlation coefficients, each across some observations of some item pair.
(See [this related answer](http://stats.stackexchange.com/questions/230479/how-to-reduce-the-dimensionality-of-a-similarity-matrix-of-categorical-co-occur/231333#231333) on why this correlation matrix is the only comparable data we have; we can't go back to raw*er* data.)

Since *both* the number of people *and* the number of item-pairs are too large to make sense of the data, we need to reduce the dimensionality.
Specifically, we want to reduce the people to *fewer* ideal types, and then describe these ideal type's *ideal types* of co-occuring items, potentially yielding of shared, categorical subjectivities of participants.

Since we are looking for a simple dimensionality reduction (not a causal or latent variable model), the n-mode generalisations of PCA, Candecomp/Parafac [PC, @carrol-chang-1970 and independently @harshman-1970] and the more involved Tucker procedures [@tucker-1966] apply here.



```{r npca, eval=FALSE, include=FALSE}
# there is candecomp == parafac (CP decomp) and Tucker3 == Multilineal SVD (+ Kroonenberg extensions)
# candecomp/parafac is notgreat, because it requires the *same* number of components for each mode

# cp vs tucker image from t M. Alex O. Vasilescu via 

#install.packages("multiway")
#install.packages("FactoMineR")
# install.packages("PTAk")
#install.packages("ThreeWay") # <- let's go with this
# library(ThreeWay)

# Candecomp/Parafac ====
cp_res <- CP(data = cora,
               laba = dimnames(cora)[[1]],
               labb = dimnames(cora)[[2]],
               labc = dimnames(cora)[[3]])
save("cp_res", file = "cp_res.Rdata")

cp_res <- load(file = "cp_res.Rdata")
cp_res
ggpairs(data = cp_res$A)
x11()
ggpairs(data = cp_res$C)

# i actually want here the lower and upper tri to be different, one for items one for people but that doesn't work bc of some label bs
# lower_tri_plot <- function(data, mapping, ...) {
#   ggplot(data = data, mapping = mapping) + geom_text()
# }

dev.next()
x11()
library(ggrepel)

for(i in ncol())
ggplot(data = as.data.frame(cp_res$A[,c(5,6)]), mapping = aes(x = Comp.5, y = Comp.6, label = rownames(cp_res$A))) + geom_point() + geom_label_repel()

rownames(cp_res$A) <- komki$items$Handle.deutsch

item_plots <- NULL
for(i in 1:ncol(cp_res$A)) {
  for(p in 1:ncol(cp_res$A)) {
    ggsave(filename = paste0("item-",i,"-",p,"-german.pdf"),
           plot = ggplot(data = as.data.frame(cp_res$A[,c(i,p)]), mapping = aes_string(x = paste0("Comp.", i), y = paste0("Comp.", p), label = "rownames(cp_res$A)")) + geom_point() + geom_label_repel(),
           width = 7,
           height = 7,
           units = "in")
  }
}
item_plots[[1]][[4]]

ggplot(data = as.data.frame(cp_res$C[,c(1,2)]), mapping = aes(x = Comp.1, y = Comp.2, label = rownames(cp_res$C))) + geom_point() + geom_text_repel()


ggpairs(data = cp_res$A, mapping = aes(label = rownames(cp_res$A)), lower = list(continuous = lower_tri_plot))
install.packages("ggrepel")

cp_res$A
cp_res$C
library(ggplot2)
library(GGally)
ggpairs(upper = )
help(ggpairs)
plotmatrix(plotdata)

all(abs(round(x = cp_res$A, digits = 1)) == abs(round(x = cp_res$B, digits = 1)))

cp_res$A

# tucker ====
tucker_res <- T3(data = cora,
                      laba = dimnames(cora)[[1]],
                      labb = dimnames(cora)[[2]],
                      labc = dimnames(cora)[[3]])
save(tucker_res, file = "tucker_res.Rdata")

tucker_res$core

ggpairs(tucker_res$A)
dev.off()
x11()

# some post prcessing as per gioardini kiers page 8
# TODO THIS STUFF FAILS
# tucker_res$A <- tucker_res$A %*% tucker_res$core
# tucker_res$core <- solve(tucker_res$core) %*% tucker_res$core

all(round(tucker_res$A, digits = 1) == round(tucker_res$B, digits = 1))
```


## Open issues

- rotation?
- pre-proccessing (centering, scaling)
- post-processing (normalizing)
- robustness and Bayesian critique (parallel analysis)


## Interpretation

While different in procedure and data type, `Q-Cat` shares the paradigmatic foundations of Q methodology.
As Watts writes about Q-*Sorts*, here *too*:

> "Subjectivity is not a mental entity. 
> It does not reflect any inner experience and it has little in common with concepts like mind and consciousness." 
>
> -- Simon Watts [-@watts2011subjectivity, p. 40]

<!-- "Operational definitions begin with concepts in search of behavior; operant definitions begin with behavior in search of concepts." [@Brown-1980, p. 28] -->

<!-- This is an extension of the scientific study of human subjectivity. -->

<!-- this is not about mental representation as from psych, but -->

<!-- -a viewpoint is relational: an interaction between subject and world (an object, another person, an event, a concept ... ) -->
<!-- -viewpoints are the act of observation -->
<!-- -viewpoints are inherently meaningful -->

<!-- ## Categorizing -->

<!-- Relevance: seeing the world efficiently through categories -->

<!-- Q-Sorting is a special case of categorization in general [@coxon-1999] -->

<!-- ## Categorizing and Subjectivity -->

<!-- Do the theory of viewpoints in operant subjectivity apply as well in categorization? -->

<!-- 	"A viewpoint does not exist within a person, but only in their current outlook or positioning relative to some aspect of their immediate environment (a circumstance perhaps, an event, or some other object of enquiry). A viewpoint exists and takes a defined form only in the moment of relationship between a subject and its object, between knower and known, observer and observed. Given this essentially relational nature, a viewpoint could never be described as belonging to a person in any enduring sense, nor could it even be made meaningful by reference to them alone." [@watts-2011, p. 40] -->


<!-- -a viewpoint is judgmental, means neither right nor wrong -->

<!-- 	"In the first place, a subjective operant, unlike a scale response, is neither right nor wrong." [@Brown-1980, p. 4] -->

<!-- -a viewpoint is the first-person perspective -->

<!-- 	"[...]Â it is of more scientific interest to know how subjects have combined items (e.g., as in Q sorting) than how subjects have responded to a set of items combined a priori by the investigator." [@Brown-1980, p. 21] -->

<!-- 	"Operational definitions begin with concepts in search of behavior; operant definitions begin with behavior in search of concepts." [@Brown-1980, p. 28] -->


<!-- -viewpoints are "categories of thinking" -->


<!-- ##Theory of Categorization -->


## References

